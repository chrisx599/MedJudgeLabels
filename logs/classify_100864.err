The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
`torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(EngineCore_DP0 pid=3808966)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=3808966)[0;0m Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:02,  1.43s/it]
[1;36m(EngineCore_DP0 pid=3808966)[0;0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.50it/s]
[1;36m(EngineCore_DP0 pid=3808966)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.02it/s]
[1;36m(EngineCore_DP0 pid=3808966)[0;0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.03it/s]
[1;36m(EngineCore_DP0 pid=3808966)[0;0m 
[1;36m(EngineCore_DP0 pid=3808966)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3808966)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3808966)[0;0m   File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_DP0 pid=3808966)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3808966)[0;0m   File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3808966)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3808966)[0;0m   File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 712, in run_engine_core
[1;36m(EngineCore_DP0 pid=3808966)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3808966)[0;0m   File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 699, in run_engine_core
[1;36m(EngineCore_DP0 pid=3808966)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3808966)[0;0m   File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 498, in __init__
[1;36m(EngineCore_DP0 pid=3808966)[0;0m     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP0 pid=3808966)[0;0m   File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 92, in __init__
[1;36m(EngineCore_DP0 pid=3808966)[0;0m     self._initialize_kv_caches(vllm_config)
[1;36m(EngineCore_DP0 pid=3808966)[0;0m   File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 199, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3808966)[0;0m     kv_cache_configs = get_kv_cache_configs(vllm_config, kv_cache_specs,
[1;36m(EngineCore_DP0 pid=3808966)[0;0m   File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/core/kv_cache_utils.py", line 1243, in get_kv_cache_configs
[1;36m(EngineCore_DP0 pid=3808966)[0;0m     check_enough_kv_cache_memory(vllm_config, kv_cache_spec_one_worker,
[1;36m(EngineCore_DP0 pid=3808966)[0;0m   File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/core/kv_cache_utils.py", line 716, in check_enough_kv_cache_memory
[1;36m(EngineCore_DP0 pid=3808966)[0;0m     raise ValueError(
[1;36m(EngineCore_DP0 pid=3808966)[0;0m ValueError: To serve at least one request with the models's max seq len (262144), (36.00 GiB KV cache is needed, which is larger than the available KV cache memory (30.88 GiB). Based on the available memory, the estimated maximum model length is 224848. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
Traceback (most recent call last):
  File "/common/home/projectgrps/CS707/CS707G2/MedJudgeLabels/classify/vllm_qwen3_4b_infer.py", line 299, in <module>
    main()
  File "/common/home/projectgrps/CS707/CS707G2/MedJudgeLabels/classify/vllm_qwen3_4b_infer.py", line 269, in main
    results = run_batch_inference(
  File "/common/home/projectgrps/CS707/CS707G2/MedJudgeLabels/classify/vllm_qwen3_4b_infer.py", line 118, in run_batch_inference
    llm = LLM(
  File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 297, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
  File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 177, in from_engine_args
    return cls(vllm_config=vllm_config,
  File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 114, in __init__
    self.engine_core = EngineCoreClient.make_client(
  File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 80, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
  File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 602, in __init__
    super().__init__(
  File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 448, in __init__
    with launch_core_engines(vllm_config, executor_class,
  File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 732, in launch_core_engines
    wait_for_engine_startup(
  File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/engine/utils.py", line 785, in wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
