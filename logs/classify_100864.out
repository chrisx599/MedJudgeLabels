Job started at: Thu 16 Oct 2025 03:48:24 PM +08
Running on node: lucas
Conda environment: med
Thu Oct 16 15:48:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40S                    On  |   00000000:01:00.0 Off |                    0 |
| N/A   35C    P8             36W /  350W |       0MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
INFO 10-16 15:48:30 [__init__.py:216] Automatically detected platform cuda.
Âä†ËΩΩpromptÊ®°Êùø: classify/classify_prompt.txt
Âä†ËΩΩÊü•ËØ¢Êï∞ÊçÆ: /common/home/projectgrps/CS707/CS707G2/PKU-SafeRLHF/data/train.jsonl
ÂÖ±Âä†ËΩΩ 73907 ‰∏™Êü•ËØ¢
Ê≠£Âú®Âä†ËΩΩÊ®°Âûã: /common/home/projectgrps/CS707/CS707G2/Qwen3-4B-Instruct-2507
ÊâπÂ§ÑÁêÜÈÖçÁΩÆ: max_num_seqs=256
INFO 10-16 15:48:34 [utils.py:233] non-default args: {'trust_remote_code': True, 'max_num_seqs': 256, 'disable_log_stats': True, 'model': '/common/home/projectgrps/CS707/CS707G2/Qwen3-4B-Instruct-2507'}
INFO 10-16 15:48:40 [model.py:547] Resolved architecture: Qwen3ForCausalLM
INFO 10-16 15:48:40 [model.py:1510] Using max model len 262144
INFO 10-16 15:48:40 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
[1;36m(EngineCore_DP0 pid=3808966)[0;0m INFO 10-16 15:48:40 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=3808966)[0;0m INFO 10-16 15:48:40 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/common/home/projectgrps/CS707/CS707G2/Qwen3-4B-Instruct-2507', speculative_config=None, tokenizer='/common/home/projectgrps/CS707/CS707G2/Qwen3-4B-Instruct-2507', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=262144, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/common/home/projectgrps/CS707/CS707G2/Qwen3-4B-Instruct-2507, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv","vllm.linear_attention","vllm.plamo2_mamba_mixer","vllm.gdn_attention","vllm.sparse_attn_indexer"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":[2,1],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3808966)[0;0m INFO 10-16 15:48:41 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3808966)[0;0m WARNING 10-16 15:48:42 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=3808966)[0;0m INFO 10-16 15:48:42 [gpu_model_runner.py:2602] Starting to load model /common/home/projectgrps/CS707/CS707G2/Qwen3-4B-Instruct-2507...
[1;36m(EngineCore_DP0 pid=3808966)[0;0m INFO 10-16 15:48:42 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=3808966)[0;0m INFO 10-16 15:48:42 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=3808966)[0;0m INFO 10-16 15:48:45 [default_loader.py:267] Loading weights took 2.98 seconds
[1;36m(EngineCore_DP0 pid=3808966)[0;0m INFO 10-16 15:48:46 [gpu_model_runner.py:2653] Model loading took 7.6065 GiB and 3.204390 seconds
[1;36m(EngineCore_DP0 pid=3808966)[0;0m INFO 10-16 15:48:51 [backends.py:548] Using cache directory: /common/home/projectgrps/CS707/CS707G2/.cache/vllm/torch_compile_cache/e7042d2e80/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=3808966)[0;0m INFO 10-16 15:48:51 [backends.py:559] Dynamo bytecode transform time: 4.52 s
[1;36m(EngineCore_DP0 pid=3808966)[0;0m INFO 10-16 15:48:54 [backends.py:197] Cache the graph for dynamic shape for later use
[1;36m(EngineCore_DP0 pid=3808966)[0;0m INFO 10-16 15:49:06 [backends.py:218] Compiling a graph for dynamic shape takes 14.66 s
[1;36m(EngineCore_DP0 pid=3808966)[0;0m INFO 10-16 15:49:35 [monitor.py:34] torch.compile takes 19.18 s in total
[1;36m(EngineCore_DP0 pid=3808966)[0;0m INFO 10-16 15:49:36 [gpu_worker.py:298] Available KV cache memory: 30.88 GiB
[1;36m(EngineCore_DP0 pid=3808966)[0;0m ERROR 10-16 15:49:37 [core.py:708] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3808966)[0;0m ERROR 10-16 15:49:37 [core.py:708] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3808966)[0;0m ERROR 10-16 15:49:37 [core.py:708]   File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 699, in run_engine_core
[1;36m(EngineCore_DP0 pid=3808966)[0;0m ERROR 10-16 15:49:37 [core.py:708]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3808966)[0;0m ERROR 10-16 15:49:37 [core.py:708]   File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 498, in __init__
[1;36m(EngineCore_DP0 pid=3808966)[0;0m ERROR 10-16 15:49:37 [core.py:708]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_DP0 pid=3808966)[0;0m ERROR 10-16 15:49:37 [core.py:708]   File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 92, in __init__
[1;36m(EngineCore_DP0 pid=3808966)[0;0m ERROR 10-16 15:49:37 [core.py:708]     self._initialize_kv_caches(vllm_config)
[1;36m(EngineCore_DP0 pid=3808966)[0;0m ERROR 10-16 15:49:37 [core.py:708]   File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 199, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3808966)[0;0m ERROR 10-16 15:49:37 [core.py:708]     kv_cache_configs = get_kv_cache_configs(vllm_config, kv_cache_specs,
[1;36m(EngineCore_DP0 pid=3808966)[0;0m ERROR 10-16 15:49:37 [core.py:708]   File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/core/kv_cache_utils.py", line 1243, in get_kv_cache_configs
[1;36m(EngineCore_DP0 pid=3808966)[0;0m ERROR 10-16 15:49:37 [core.py:708]     check_enough_kv_cache_memory(vllm_config, kv_cache_spec_one_worker,
[1;36m(EngineCore_DP0 pid=3808966)[0;0m ERROR 10-16 15:49:37 [core.py:708]   File "/common/home/projectgrps/CS707/CS707G2/.conda/envs/med/lib/python3.10/site-packages/vllm/v1/core/kv_cache_utils.py", line 716, in check_enough_kv_cache_memory
[1;36m(EngineCore_DP0 pid=3808966)[0;0m ERROR 10-16 15:49:37 [core.py:708]     raise ValueError(
[1;36m(EngineCore_DP0 pid=3808966)[0;0m ERROR 10-16 15:49:37 [core.py:708] ValueError: To serve at least one request with the models's max seq len (262144), (36.00 GiB KV cache is needed, which is larger than the available KV cache memory (30.88 GiB). Based on the available memory, the estimated maximum model length is 224848. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
Job finished at: Thu 16 Oct 2025 03:49:38 PM +08
